{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54844d53-5452-4a5f-b0c0-f1c7563ff61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the future... the future looks very different. You've got to fight the good fight. It's not going to be easy.\"\n",
      "\n",
      "What could be easier to fight than the next man in his life?\n",
      "\n",
      "\"You know, I've got to fight it. I'm going to fight it. I'm going to be the best in the world.\"\n",
      "\n",
      "And what would be his next move?\n",
      "\n",
      "\"You know, I don't want to do it.\"\n",
      "\n",
      "I'm not going to take my life.\n",
      "\n",
      "\"You know, I think I want to. I want to be the best in the world. I want to be the best in my life. I want to be the best in my life\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Ładowanie modelu GPT-2 oraz tokenizera\n",
    "model_name = \"gpt2\"  # Możesz także użyć 'gpt2-medium', 'gpt2-large'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "def generate_text(prompt, max_length=100, temperature=1.0, top_k=50):\n",
    "    # Tokenizowanie promptu (tekstu wejściowego)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generowanie tekstu na podstawie promptu\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Dekodowanie wygenerowanego tekstu\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Przykład użycia generatora tekstu\n",
    "prompt = \"In the future...\"\n",
    "generated_text = generate_text(prompt, max_length=150, temperature=0.7, top_k=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045c3af-310b-467f-8686-d05364cd2fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
